{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "#connect with weights and biases api\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# all lightfm imports \n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm import cross_validation\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "# using a customer history based and astrologer categorical segmentation based hybrid model\n",
    "\n",
    "# imports re for text cleaning \n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# we will ignore pandas warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((0, 0))\n",
    "input_path = \"/numpy_data.pkl\"\n",
    "interp_files=[]\n",
    "for dir in os.listdir(input_path):\n",
    "    for file in os.listdir(input_path + '/' + dir):\n",
    "        if file.endswith(\".npy\"):\n",
    "            interp_files.append(input_path + '/' + dir + '/' + file)\n",
    "\n",
    "for file in interp_files:\n",
    "    temp = np.load(file)\n",
    "    # temp = temp.reshape((4800,11,1))\n",
    "    x = np.append(x, temp, axis=0)\n",
    "\n",
    "x = x[4800:, :]\n",
    "print(x.shape)\n",
    "x_vals = x[:, :2]\n",
    "y_vals = x[:, 2:]\n",
    "x_vals = x_vals.reshape((38400, 2, 1))\n",
    "y_vals = y_vals.reshape((38400, 9, 1))\n",
    "print(x_vals.shape + y_vals.shape)\n",
    "# Splitting data into train, test and validation data\n",
    "n = 38400\n",
    "x_train = x_vals[0:int(0.7*n), :, :]\n",
    "y_train = y_vals[0:int(0.7*n), :, :]\n",
    "x_test = x_vals[int(0.7*n):, :, :]\n",
    "y_test = y_vals[int(0.7*n):, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "def data_load():\n",
    "    x = np.zeros((4800,11))\n",
    "    input_path = \"content/numpy_data\"\n",
    "    interp_files=[]\n",
    "    for dir in os.listdir(input_path):\n",
    "        for file in os.listdir(input_path + '/' + dir):\n",
    "            if file.endswith(\".npy\"):\n",
    "                interp_files.append(input_path + '/' + dir + '/' + file)\n",
    "\n",
    "    for file in interp_files:\n",
    "        temp = np.load(file)\n",
    "        # temp = temp.reshape((4800,11,1))\n",
    "        x = np.append(x, temp, axis=0)\n",
    "\n",
    "    x = x[4800:, :]\n",
    "    x_vals = x[:, :9]\n",
    "    y_vals = x[:, 9:]\n",
    "    x_vals = x_vals.reshape((38400, 2, 1))\n",
    "    y_vals = y_vals.reshape((38400, 9, 1))\n",
    "    print(x_vals.shape + y_vals.shape)\n",
    "\n",
    "    n = 38400\n",
    "    x_train = x_vals[0:int(0.7*n), :, :]\n",
    "    y_train = y_vals[0:int(0.7*n), :, :]\n",
    "    x_test = x_vals[int(0.7*n):, :, :]\n",
    "    y_test = y_vals[int(0.7*n):, :, :]\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = data_load()\n",
    "\n",
    "sweep_config = {\n",
    "  'method': 'bayes', \n",
    "  'metric': {\n",
    "      'name': 'val_loss',\n",
    "      'goal': 'minimize'\n",
    "  },\n",
    "  'early_terminate':{\n",
    "      'type': 'hyperband',\n",
    "      'min_iter': 5\n",
    "  },\n",
    "  'parameters': {\n",
    "      'batch_size': {\n",
    "          'values': [32]\n",
    "      },\n",
    "      'learning_rate':{\n",
    "          'values': [0.0001]\n",
    "      },\n",
    "      'neurons':{\n",
    "          'values': [32, 64]\n",
    "      },\n",
    "      'activation':{\n",
    "          'values': ['tanh', 'relu']\n",
    "      }\n",
    "  }\n",
    "}\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(wandb.config.neurons, activation=wandb.config.activation, input_shape=(2, 1)))\n",
    "    model.add(tf.keras.layers.RepeatVector(9))\n",
    "    model.add(tf.keras.layers.LSTM(wandb.config.neurons, activation=wandb.config.activation, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(wandb.config.neurons, activation=wandb.config.activation, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(wandb.config.neurons, activation=wandb.config.activation, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(wandb.config.neurons, activation=wandb.config.activation, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(wandb.config.neurons, activation=wandb.config.activation, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(wandb.config.neurons, activation=wandb.config.activation, return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(wandb.config.neurons, activation=wandb.config.activation, return_sequences=True))\n",
    "    model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)))\n",
    "    # model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def train():\n",
    "    # Specify the hyperparameter to be tuned along with\n",
    "    # an initial value\n",
    "    config_defaults = {\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.0001,\n",
    "        'neurons': 32,\n",
    "        'activation': 'tanh'\n",
    "    }\n",
    "\n",
    "    # Initialize wandb with a sample project name\n",
    "    wandb.init(config=config_defaults)\n",
    "\n",
    "    # Specify the other hyperparameters to the configuration, if any\n",
    "    wandb.config.epochs = 300\n",
    "\n",
    "    # Prepare trainloader\n",
    "    trainloader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    trainloader = trainloader.shuffle(1024).batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    # prepare testloader \n",
    "    testloader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    testloader = testloader.batch(wandb.config.batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Iniialize model with hyperparameters\n",
    "    keras.backend.clear_session()\n",
    "    model = get_compiled_model()\n",
    "    \n",
    "    # Compile the model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=wandb.config.learning_rate) # optimizer with different learning rate specified by config\n",
    "    model.compile(opt, metrics=['acc'], loss='mse')\n",
    "    \n",
    "    # Train the model\n",
    "    _ = model.fit(trainloader,\n",
    "                  epochs=wandb.config.epochs, \n",
    "                  validation_data=testloader,\n",
    "                  callbacks=[WandbCallback()]) # WandbCallback to automatically track metrics\n",
    "                            \n",
    "    # Evaluate    \n",
    "    loss, accuracy = model.evaluate(testloader, callbacks=[WandbCallback()])\n",
    "    print('Test Error Rate: ', round((1-accuracy)*100, 2))\n",
    "    wandb.log({'Test Error Rate': round((1-accuracy)*100, 2)}) # wandb.log to track custom metrics\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"ddp-second_run\", entity=\"ddp_profpatra\")\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.plot(df['Step'], df['swept-sweep-5 - val_loss'], label='tanh - 64 neurons')\n",
    "plt.plot(df['Step'], df['divine-sweep-3 - val_loss'], label='relu - 64 neurons')\n",
    "plt.plot(df['Step'], df['twilight-sweep-2 - val_loss'], label='tanh - 32 neurons')\n",
    "plt.plot(df['Step'], df['glad-sweep-1 - val_loss'], label='relu - 32 neurons')\n",
    "# plt.plot(df['Step'], df['vague-sweep-1 - val_loss'], '--', label='Upto 8% strain as input')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('No. of epochs')\n",
    "# plt.scatter(eff_strain, vonmises, label='LSTM Predictions')\n",
    "plt.legend()\n",
    "plt.savefig('300_epoch_train.png', dpi=700, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "divine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "005040abc8c1ce59866d9ed7d7c7b48b1f03ecb1c960b5fa35521eb3fca7acfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
